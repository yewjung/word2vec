{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "This is known as irony\n",
      "The US is amazing\n",
      "Those couples are still together but Jayre and Carolyn called it quitsThis is my world\n",
      "HAPPY BIRTHDAY KATE\n",
      "Welcome to my world\n",
      "Everything is fine\n",
      "Is ther anything i can do for you\n",
      "He's one a kind\n",
      "There's something you should know i'm here but he is not there\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "raw_text = \"Hello. (This is known as irony.) The U.S. is amazing. Those couples are still together, but Jayre and Carolyn called it quits.This is my world. HAPPY BIRTHDAY KATE!!!!! Welcome to my world. Everything is fine? Is ther anything i can do for you. He's one a kind. There's something you should know: i'm here; but he is not there.\"\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Remove punctuation marks from the sentence\n",
    "    # cleaned_sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # cleaned_sentence = re.sub(r\"[^\\w\\s']+\", \"\", sentence)\n",
    "    return re.sub(r\"[^\\w\\s']+\", \"\", sentence)\n",
    "\n",
    "def text_to_sentences(text):\n",
    "    # Split the text into sentences using regular expressions\n",
    "    text_without_parentheses = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text_without_parentheses)\n",
    "    return sentences\n",
    "\n",
    "# for sent in sents:\n",
    "#     print(sent)\n",
    "\n",
    "# print()\n",
    "# for sent in map(clean_sentences, sents):\n",
    "#     print(sent)\n",
    "# for sent in sents:\n",
    "#     print(sent)\n",
    "\n",
    "\n",
    "\n",
    "def get_sentences(raw_text):\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    return map(clean_sentence, sentences)\n",
    "for s in get_sentences(raw_text):\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something similar should happen in religious debate to people who talk about Torahbased Judaism\n"
     ]
    }
   ],
   "source": [
    "def extract_posts_from_xml(xml_content) -> str:\n",
    "    posts = []\n",
    "    \n",
    "    # Parse the XML content\n",
    "    xml_content = re.sub(r'&nbsp;', ' ', xml_content)\n",
    "    root = ET.fromstring(xml_content)\n",
    "    \n",
    "    # Find all <post> elements\n",
    "    post_elements = root.findall('.//post')\n",
    "    \n",
    "    # Extract the text from each <post> element\n",
    "    for post_element in post_elements:\n",
    "        post_text = post_element.text.strip()\n",
    "        posts.append(post_text)\n",
    "    \n",
    "    return \" \".join(posts)\n",
    "\n",
    "filename = \"./blogs/980769.male.25.indUnk.Capricorn.xml\"\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "posts = extract_posts_from_xml(text)\n",
    "all_sentences = list(get_sentences(posts))\n",
    "print(all_sentences[1])\n",
    "words = all_sentences[1].split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for x, y in zip(xs, ys):\n",
    "#     print(f\"{x} --> {y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_param=41942\n",
      "step: 0 training loss: 7.6205 valuation loss: 7.6199\n",
      "step: 100 training loss: 7.6085 valuation loss: 7.6068\n",
      "step: 200 training loss: 7.5877 valuation loss: 7.5989\n",
      "step: 300 training loss: 7.5826 valuation loss: 7.5894\n",
      "step: 400 training loss: 7.5851 valuation loss: 7.5856\n",
      "step: 500 training loss: 7.5643 valuation loss: 7.5693\n",
      "step: 600 training loss: 7.5459 valuation loss: 7.5555\n",
      "step: 700 training loss: 7.5390 valuation loss: 7.5478\n",
      "step: 800 training loss: 7.5235 valuation loss: 7.5556\n",
      "step: 900 training loss: 7.5178 valuation loss: 7.5333\n",
      "step: 1000 training loss: 7.4965 valuation loss: 7.5240\n",
      "step: 1100 training loss: 7.5007 valuation loss: 7.5193\n",
      "step: 1200 training loss: 7.4807 valuation loss: 7.5013\n",
      "step: 1300 training loss: 7.4604 valuation loss: 7.4870\n",
      "step: 1400 training loss: 7.4437 valuation loss: 7.4848\n",
      "step: 1500 training loss: 7.4275 valuation loss: 7.4592\n",
      "step: 1600 training loss: 7.4120 valuation loss: 7.4418\n",
      "step: 1700 training loss: 7.3990 valuation loss: 7.4353\n",
      "step: 1800 training loss: 7.3719 valuation loss: 7.4038\n",
      "step: 1900 training loss: 7.3348 valuation loss: 7.3773\n",
      "step: 2000 training loss: 7.3304 valuation loss: 7.3875\n",
      "step: 2100 training loss: 7.2884 valuation loss: 7.3679\n",
      "step: 2200 training loss: 7.2997 valuation loss: 7.3713\n",
      "step: 2300 training loss: 7.2427 valuation loss: 7.3129\n",
      "step: 2400 training loss: 7.2326 valuation loss: 7.2887\n",
      "step: 2500 training loss: 7.1967 valuation loss: 7.2378\n",
      "step: 2600 training loss: 7.1333 valuation loss: 7.2399\n",
      "step: 2700 training loss: 7.1175 valuation loss: 7.1837\n",
      "step: 2800 training loss: 7.0620 valuation loss: 7.1666\n",
      "step: 2900 training loss: 7.0790 valuation loss: 7.1589\n",
      "step: 3000 training loss: 7.0078 valuation loss: 7.1021\n",
      "step: 3100 training loss: 6.9631 valuation loss: 7.0715\n",
      "step: 3200 training loss: 6.8845 valuation loss: 7.0528\n",
      "step: 3300 training loss: 6.8988 valuation loss: 7.0111\n",
      "step: 3400 training loss: 6.8842 valuation loss: 7.0091\n",
      "step: 3500 training loss: 6.9161 valuation loss: 7.0318\n",
      "step: 3600 training loss: 6.8505 valuation loss: 7.0397\n",
      "step: 3700 training loss: 6.8688 valuation loss: 6.9893\n",
      "step: 3800 training loss: 6.7720 valuation loss: 6.9843\n",
      "step: 3900 training loss: 6.8289 valuation loss: 6.9291\n",
      "step: 4000 training loss: 6.7720 valuation loss: 6.9683\n",
      "step: 4100 training loss: 6.7870 valuation loss: 6.9899\n",
      "step: 4200 training loss: 6.7503 valuation loss: 6.9199\n",
      "step: 4300 training loss: 6.6828 valuation loss: 6.9312\n",
      "step: 4400 training loss: 6.7692 valuation loss: 6.9527\n",
      "step: 4500 training loss: 6.6959 valuation loss: 6.9247\n",
      "step: 4600 training loss: 6.6308 valuation loss: 6.8793\n",
      "step: 4700 training loss: 6.6788 valuation loss: 6.8677\n",
      "step: 4800 training loss: 6.6733 valuation loss: 6.8746\n",
      "step: 4900 training loss: 6.6265 valuation loss: 6.8766\n",
      "step: 5000 training loss: 6.6456 valuation loss: 6.8815\n",
      "step: 5100 training loss: 6.6210 valuation loss: 6.8845\n",
      "step: 5200 training loss: 6.6095 valuation loss: 6.8952\n",
      "step: 5300 training loss: 6.5720 valuation loss: 6.8364\n",
      "step: 5400 training loss: 6.5697 valuation loss: 6.8815\n",
      "step: 5500 training loss: 6.6075 valuation loss: 6.8254\n",
      "step: 5600 training loss: 6.5639 valuation loss: 6.8640\n",
      "step: 5700 training loss: 6.5879 valuation loss: 6.8193\n",
      "step: 5800 training loss: 6.4940 valuation loss: 6.9120\n",
      "step: 5900 training loss: 6.4795 valuation loss: 6.8244\n",
      "step: 6000 training loss: 6.5388 valuation loss: 6.8134\n",
      "step: 6100 training loss: 6.5192 valuation loss: 6.8635\n",
      "step: 6200 training loss: 6.4461 valuation loss: 6.8330\n",
      "step: 6300 training loss: 6.3916 valuation loss: 6.7625\n",
      "step: 6400 training loss: 6.4189 valuation loss: 6.8448\n",
      "step: 6500 training loss: 6.4611 valuation loss: 6.8017\n",
      "step: 6600 training loss: 6.4308 valuation loss: 6.7955\n",
      "step: 6700 training loss: 6.4639 valuation loss: 6.7924\n",
      "step: 6800 training loss: 6.3764 valuation loss: 6.7955\n",
      "step: 6900 training loss: 6.4123 valuation loss: 6.7340\n",
      "step: 7000 training loss: 6.3434 valuation loss: 6.8026\n",
      "step: 7100 training loss: 6.4639 valuation loss: 6.8269\n",
      "step: 7200 training loss: 6.3722 valuation loss: 6.8439\n",
      "step: 7300 training loss: 6.3753 valuation loss: 6.7969\n",
      "step: 7400 training loss: 6.3670 valuation loss: 6.7876\n",
      "step: 7500 training loss: 6.3974 valuation loss: 6.7509\n",
      "step: 7600 training loss: 6.3255 valuation loss: 6.7848\n",
      "step: 7700 training loss: 6.3704 valuation loss: 6.8323\n",
      "step: 7800 training loss: 6.3153 valuation loss: 6.7609\n",
      "step: 7900 training loss: 6.3421 valuation loss: 6.7554\n",
      "step: 8000 training loss: 6.3430 valuation loss: 6.8191\n",
      "step: 8100 training loss: 6.2658 valuation loss: 6.7998\n",
      "step: 8200 training loss: 6.2470 valuation loss: 6.7468\n",
      "step: 8300 training loss: 6.3360 valuation loss: 6.8032\n",
      "step: 8400 training loss: 6.2518 valuation loss: 6.7835\n",
      "step: 8500 training loss: 6.2894 valuation loss: 6.8070\n",
      "step: 8600 training loss: 6.2864 valuation loss: 6.7609\n",
      "step: 8700 training loss: 6.2580 valuation loss: 6.7891\n",
      "step: 8800 training loss: 6.2878 valuation loss: 6.7779\n",
      "step: 8900 training loss: 6.3105 valuation loss: 6.8160\n",
      "step: 9000 training loss: 6.2138 valuation loss: 6.7936\n",
      "step: 9100 training loss: 6.2451 valuation loss: 6.8277\n",
      "step: 9200 training loss: 6.2268 valuation loss: 6.7097\n",
      "step: 9300 training loss: 6.2738 valuation loss: 6.7485\n",
      "step: 9400 training loss: 6.2431 valuation loss: 6.7918\n",
      "step: 9500 training loss: 6.1951 valuation loss: 6.7908\n",
      "step: 9600 training loss: 6.1751 valuation loss: 6.8072\n",
      "step: 9700 training loss: 6.2008 valuation loss: 6.7130\n",
      "step: 9800 training loss: 6.1905 valuation loss: 6.8449\n",
      "step: 9900 training loss: 6.1011 valuation loss: 6.7966\n",
      "step: 10000 training loss: 6.1474 valuation loss: 6.7078\n",
      "step: 10100 training loss: 6.2355 valuation loss: 6.7509\n",
      "step: 10200 training loss: 6.1500 valuation loss: 6.7060\n",
      "step: 10300 training loss: 6.1106 valuation loss: 6.7549\n",
      "step: 10400 training loss: 6.1652 valuation loss: 6.7222\n",
      "step: 10500 training loss: 6.1561 valuation loss: 6.7321\n",
      "step: 10600 training loss: 6.1173 valuation loss: 6.8077\n",
      "step: 10700 training loss: 6.1397 valuation loss: 6.7931\n",
      "step: 10800 training loss: 6.1448 valuation loss: 6.7481\n",
      "step: 10900 training loss: 6.1663 valuation loss: 6.7710\n",
      "step: 11000 training loss: 6.1561 valuation loss: 6.8650\n",
      "step: 11100 training loss: 6.1462 valuation loss: 6.8007\n",
      "step: 11200 training loss: 6.1276 valuation loss: 6.8095\n",
      "step: 11300 training loss: 6.0973 valuation loss: 6.8156\n",
      "step: 11400 training loss: 6.0836 valuation loss: 6.7716\n",
      "step: 11500 training loss: 6.1023 valuation loss: 6.8191\n",
      "step: 11600 training loss: 6.0666 valuation loss: 6.7840\n",
      "step: 11700 training loss: 6.0874 valuation loss: 6.7572\n",
      "step: 11800 training loss: 6.0406 valuation loss: 6.7779\n",
      "step: 11900 training loss: 6.1100 valuation loss: 6.8757\n",
      "step: 12000 training loss: 6.0515 valuation loss: 6.8307\n",
      "step: 12100 training loss: 6.0338 valuation loss: 6.7647\n",
      "step: 12200 training loss: 6.1048 valuation loss: 6.7431\n",
      "step: 12300 training loss: 6.0613 valuation loss: 6.7963\n",
      "step: 12400 training loss: 6.0599 valuation loss: 6.7881\n",
      "step: 12500 training loss: 6.0713 valuation loss: 6.7890\n",
      "step: 12600 training loss: 6.0560 valuation loss: 6.8253\n",
      "step: 12700 training loss: 6.0380 valuation loss: 6.8008\n",
      "step: 12800 training loss: 6.0426 valuation loss: 6.8247\n",
      "step: 12900 training loss: 5.9429 valuation loss: 6.8749\n",
      "step: 13000 training loss: 6.0793 valuation loss: 6.8608\n",
      "step: 13100 training loss: 6.0351 valuation loss: 6.8260\n",
      "step: 13200 training loss: 5.9879 valuation loss: 6.8661\n",
      "step: 13300 training loss: 6.0285 valuation loss: 6.8563\n",
      "step: 13400 training loss: 5.9750 valuation loss: 6.8173\n",
      "step: 13500 training loss: 6.0201 valuation loss: 6.8311\n",
      "step: 13600 training loss: 6.0059 valuation loss: 6.8402\n",
      "step: 13700 training loss: 6.0309 valuation loss: 6.8543\n",
      "step: 13800 training loss: 6.0027 valuation loss: 6.8300\n",
      "step: 13900 training loss: 6.0022 valuation loss: 6.8599\n",
      "step: 14000 training loss: 5.9915 valuation loss: 6.8851\n",
      "step: 14100 training loss: 5.9510 valuation loss: 6.8450\n",
      "step: 14200 training loss: 5.9336 valuation loss: 6.8333\n",
      "step: 14300 training loss: 6.0330 valuation loss: 6.8692\n",
      "step: 14400 training loss: 5.9753 valuation loss: 6.8522\n",
      "step: 14500 training loss: 5.9597 valuation loss: 6.9000\n",
      "step: 14600 training loss: 5.9568 valuation loss: 6.8474\n",
      "step: 14700 training loss: 5.9610 valuation loss: 6.8709\n",
      "step: 14800 training loss: 5.9326 valuation loss: 6.8443\n",
      "step: 14900 training loss: 5.9482 valuation loss: 6.9096\n",
      "step: 15000 training loss: 5.9326 valuation loss: 6.8100\n",
      "step: 15100 training loss: 5.9099 valuation loss: 6.8277\n",
      "step: 15200 training loss: 5.9374 valuation loss: 6.9195\n",
      "step: 15300 training loss: 5.9636 valuation loss: 6.8759\n",
      "step: 15400 training loss: 5.9654 valuation loss: 6.8377\n",
      "step: 15500 training loss: 5.9461 valuation loss: 6.9274\n",
      "step: 15600 training loss: 5.9207 valuation loss: 6.8661\n",
      "step: 15700 training loss: 5.8997 valuation loss: 6.8304\n",
      "step: 15800 training loss: 5.8815 valuation loss: 6.8795\n",
      "step: 15900 training loss: 5.9250 valuation loss: 6.8560\n",
      "step: 16000 training loss: 5.8805 valuation loss: 6.8752\n",
      "step: 16100 training loss: 5.9491 valuation loss: 6.9028\n",
      "step: 16200 training loss: 5.9503 valuation loss: 6.9279\n",
      "step: 16300 training loss: 5.8620 valuation loss: 6.8640\n",
      "step: 16400 training loss: 5.9043 valuation loss: 6.9827\n",
      "step: 16500 training loss: 5.8755 valuation loss: 6.8712\n",
      "step: 16600 training loss: 5.8488 valuation loss: 6.9193\n",
      "step: 16700 training loss: 5.8968 valuation loss: 6.9601\n",
      "step: 16800 training loss: 5.9036 valuation loss: 6.9897\n",
      "step: 16900 training loss: 5.8630 valuation loss: 6.9873\n",
      "step: 17000 training loss: 5.8639 valuation loss: 6.8954\n",
      "step: 17100 training loss: 5.8664 valuation loss: 6.9242\n",
      "step: 17200 training loss: 5.8680 valuation loss: 6.9060\n",
      "step: 17300 training loss: 5.7803 valuation loss: 7.0181\n",
      "step: 17400 training loss: 5.8342 valuation loss: 6.8887\n",
      "step: 17500 training loss: 5.8267 valuation loss: 6.9023\n",
      "step: 17600 training loss: 5.8391 valuation loss: 6.9526\n",
      "step: 17700 training loss: 5.7950 valuation loss: 6.9539\n",
      "step: 17800 training loss: 5.8376 valuation loss: 6.9805\n",
      "step: 17900 training loss: 5.7883 valuation loss: 6.9884\n",
      "step: 18000 training loss: 5.8261 valuation loss: 6.9848\n",
      "step: 18100 training loss: 5.7673 valuation loss: 6.9527\n",
      "step: 18200 training loss: 5.7473 valuation loss: 6.9444\n",
      "step: 18300 training loss: 5.8752 valuation loss: 6.9241\n",
      "step: 18400 training loss: 5.7862 valuation loss: 6.9849\n",
      "step: 18500 training loss: 5.7775 valuation loss: 6.9658\n",
      "step: 18600 training loss: 5.7556 valuation loss: 6.9326\n",
      "step: 18700 training loss: 5.8517 valuation loss: 6.9545\n",
      "step: 18800 training loss: 5.7909 valuation loss: 6.9972\n",
      "step: 18900 training loss: 5.8484 valuation loss: 6.9646\n",
      "step: 19000 training loss: 5.7563 valuation loss: 6.9138\n",
      "step: 19100 training loss: 5.7259 valuation loss: 6.9501\n",
      "step: 19200 training loss: 5.7943 valuation loss: 6.9528\n",
      "step: 19300 training loss: 5.6998 valuation loss: 7.0207\n",
      "step: 19400 training loss: 5.7768 valuation loss: 7.0027\n",
      "step: 19500 training loss: 5.7129 valuation loss: 7.0426\n",
      "step: 19600 training loss: 5.7103 valuation loss: 6.9774\n",
      "step: 19700 training loss: 5.7519 valuation loss: 6.9700\n",
      "step: 19800 training loss: 5.7833 valuation loss: 7.0463\n",
      "step: 19900 training loss: 5.7954 valuation loss: 7.0579\n",
      "step: 19999 training loss: 5.6982 valuation loss: 6.9657\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# creating vocab\n",
    "vocab_set = set()\n",
    "sentences = all_sentences\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        vocab_set.add(word)\n",
    "vocab = sorted(list(vocab_set))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "w2i = {w:i for i, w in enumerate(vocab)}\n",
    "side_spread = 2\n",
    "batch_size = 32\n",
    "n_embed = 10\n",
    "n_hidden = 10\n",
    "\n",
    "\n",
    "def get_context_n_target(words):\n",
    "    X, Y = [], []\n",
    "    if len(words) < side_spread + 1:\n",
    "        return X, Y\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        l = max(0, i - side_spread)\n",
    "        r = min(i + side_spread, len(words) - 1)\n",
    "        l = max(l, i - (r - i))\n",
    "        r = min(r, i + (i - l))\n",
    "        if i == r:\n",
    "            continue\n",
    "        x = list(map(w2i.get, [*words[l:i], *words[i+1:r+1]]))\n",
    "        X.append(x)\n",
    "\n",
    "        y = w2i[words[i]]\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def get_x_y(words):\n",
    "    window = 2*side_spread + 1\n",
    "    if len(words) < window:\n",
    "        return [], []\n",
    "    x, y = [], []\n",
    "    for i in range(len(words) - window + 1):\n",
    "        context_words = [*words[i:i+side_spread], *words[i+side_spread+1:i+2*side_spread+1]]\n",
    "        target_word = words[i+side_spread]\n",
    "        x.append([w2i[w] for w in context_words])\n",
    "        y.append(w2i[target_word])\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "\n",
    "\n",
    "def get_all_data(sentences: List[str]):\n",
    "    all_x, all_y = [], []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        x, y = get_x_y(words)\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "    return torch.tensor(all_x), torch.tensor(all_y)\n",
    "\n",
    "# splitting training and validation sets\n",
    "X, Y = get_all_data(all_sentences)\n",
    "# X, Y = get_context_n_target(words)\n",
    "n = int(0.9 * vocab_size)\n",
    "train_x, train_y = X[:n], Y[:n]\n",
    "val_x, val_y = X[n:], Y[n:]\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    x, y = (train_x, train_y) if 'train' == split else (val_x, val_y)\n",
    "    idx = torch.randint(len(x), (batch_size, ), generator=g)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target: torch.Tensor = None):\n",
    "        embeddings: torch.Tensor = self.embedding(x)\n",
    "        summed = embeddings.mean(1, keepdim=False) # (B, n_embed)\n",
    "        logits = self.proj(summed) # (B, vocab_size)\n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, eval_iters: int):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "epochs = 20000\n",
    "def training(eval_iters: int):\n",
    "    m = CBOW()\n",
    "    n_param = sum(p.numel() for p in m.parameters())\n",
    "    print(f'{n_param=}')\n",
    "    optim = torch.optim.SGD(m.parameters(), lr=1e-2)\n",
    "    for i in range(epochs):\n",
    "        # get batch\n",
    "        x, y = get_batch('train')\n",
    "\n",
    "        # forward\n",
    "        _, loss = m(x, y)\n",
    "        if i % eval_iters == 0 or i == epochs - 1:\n",
    "            losses = estimate_loss(m, eval_iters)\n",
    "            print(f\"step: {i} training loss: {losses['train']:.4f} valuation loss: {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "        # backward\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optim.step()\n",
    "    \n",
    "    return m\n",
    "\n",
    "m = training(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5176, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
