{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "This is known as irony\n",
      "The US is amazing\n",
      "Those couples are still together but Jayre and Carolyn called it quitsThis is my world\n",
      "HAPPY BIRTHDAY KATE\n",
      "Welcome to my world\n",
      "Everything is fine\n",
      "Is ther anything i can do for you\n",
      "He's one a kind\n",
      "There's something you should know i'm here but he is not there\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "raw_text = \"Hello. (This is known as irony.) The U.S. is amazing. Those couples are still together, but Jayre and Carolyn called it quits.This is my world. HAPPY BIRTHDAY KATE!!!!! Welcome to my world. Everything is fine? Is ther anything i can do for you. He's one a kind. There's something you should know: i'm here; but he is not there.\"\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Remove punctuation marks from the sentence\n",
    "    # cleaned_sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # cleaned_sentence = re.sub(r\"[^\\w\\s']+\", \"\", sentence)\n",
    "    return re.sub(r\"[^\\w\\s']+\", \"\", sentence)\n",
    "\n",
    "def text_to_sentences(text):\n",
    "    # Split the text into sentences using regular expressions\n",
    "    text_without_parentheses = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text_without_parentheses)\n",
    "    return sentences\n",
    "\n",
    "# for sent in sents:\n",
    "#     print(sent)\n",
    "\n",
    "# print()\n",
    "# for sent in map(clean_sentences, sents):\n",
    "#     print(sent)\n",
    "# for sent in sents:\n",
    "#     print(sent)\n",
    "\n",
    "\n",
    "\n",
    "def get_sentences(raw_text):\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    return map(clean_sentence, sentences)\n",
    "for s in get_sentences(raw_text):\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something similar should happen in religious debate to people who talk about Torahbased Judaism\n"
     ]
    }
   ],
   "source": [
    "def extract_posts_from_xml(xml_content) -> str:\n",
    "    posts = []\n",
    "    \n",
    "    # Parse the XML content\n",
    "    xml_content = re.sub(r'&nbsp;', ' ', xml_content)\n",
    "    root = ET.fromstring(xml_content)\n",
    "    \n",
    "    # Find all <post> elements\n",
    "    post_elements = root.findall('.//post')\n",
    "    \n",
    "    # Extract the text from each <post> element\n",
    "    for post_element in post_elements:\n",
    "        post_text = post_element.text.strip()\n",
    "        posts.append(post_text)\n",
    "    \n",
    "    return \" \".join(posts)\n",
    "\n",
    "filename = \"./blogs/980769.male.25.indUnk.Capricorn.xml\"\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "posts = extract_posts_from_xml(text)\n",
    "all_sentences = list(get_sentences(posts))\n",
    "print(all_sentences[1])\n",
    "words = all_sentences[1].split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for x, y in zip(xs, ys):\n",
    "#     print(f\"{x} --> {y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 training loss: 7.6365 valuation loss: 7.6253\n",
      "step: 100 training loss: 7.6176 valuation loss: 7.6072\n",
      "step: 200 training loss: 7.6027 valuation loss: 7.5901\n",
      "step: 300 training loss: 7.5965 valuation loss: 7.5740\n",
      "step: 400 training loss: 7.5776 valuation loss: 7.5609\n",
      "step: 500 training loss: 7.5675 valuation loss: 7.5590\n",
      "step: 600 training loss: 7.5440 valuation loss: 7.5273\n",
      "step: 700 training loss: 7.5161 valuation loss: 7.5310\n",
      "step: 800 training loss: 7.5090 valuation loss: 7.5210\n",
      "step: 900 training loss: 7.4945 valuation loss: 7.4980\n",
      "step: 1000 training loss: 7.4777 valuation loss: 7.4879\n",
      "step: 1100 training loss: 7.4865 valuation loss: 7.4879\n",
      "step: 1200 training loss: 7.4715 valuation loss: 7.4608\n",
      "step: 1300 training loss: 7.4401 valuation loss: 7.4321\n",
      "step: 1400 training loss: 7.4095 valuation loss: 7.4303\n",
      "step: 1500 training loss: 7.3977 valuation loss: 7.4064\n",
      "step: 1600 training loss: 7.3541 valuation loss: 7.3870\n",
      "step: 1700 training loss: 7.3504 valuation loss: 7.3780\n",
      "step: 1800 training loss: 7.3206 valuation loss: 7.3363\n",
      "step: 1900 training loss: 7.2698 valuation loss: 7.3026\n",
      "step: 2000 training loss: 7.2544 valuation loss: 7.3049\n",
      "step: 2100 training loss: 7.2132 valuation loss: 7.2831\n",
      "step: 2200 training loss: 7.2253 valuation loss: 7.2908\n",
      "step: 2300 training loss: 7.1621 valuation loss: 7.2090\n",
      "step: 2400 training loss: 7.1178 valuation loss: 7.1744\n",
      "step: 2500 training loss: 7.1041 valuation loss: 7.1422\n",
      "step: 2600 training loss: 7.0265 valuation loss: 7.1387\n",
      "step: 2700 training loss: 7.0100 valuation loss: 7.0917\n",
      "step: 2800 training loss: 6.9938 valuation loss: 7.0826\n",
      "step: 2900 training loss: 7.0093 valuation loss: 7.0868\n",
      "step: 3000 training loss: 6.9332 valuation loss: 7.0264\n",
      "step: 3100 training loss: 6.8984 valuation loss: 7.0005\n",
      "step: 3200 training loss: 6.8108 valuation loss: 7.0020\n",
      "step: 3300 training loss: 6.8374 valuation loss: 6.9486\n",
      "step: 3400 training loss: 6.8331 valuation loss: 6.9633\n",
      "step: 3500 training loss: 6.8440 valuation loss: 6.9554\n",
      "step: 3600 training loss: 6.8110 valuation loss: 6.9995\n",
      "step: 3700 training loss: 6.8285 valuation loss: 6.9273\n",
      "step: 3800 training loss: 6.7335 valuation loss: 6.9510\n",
      "step: 3900 training loss: 6.7961 valuation loss: 6.9174\n",
      "step: 4000 training loss: 6.7358 valuation loss: 6.9322\n",
      "step: 4100 training loss: 6.7350 valuation loss: 6.9650\n",
      "step: 4200 training loss: 6.7150 valuation loss: 6.8735\n",
      "step: 4300 training loss: 6.6597 valuation loss: 6.8893\n",
      "step: 4400 training loss: 6.7373 valuation loss: 6.9198\n",
      "step: 4500 training loss: 6.6629 valuation loss: 6.8852\n",
      "step: 4600 training loss: 6.6061 valuation loss: 6.8622\n",
      "step: 4700 training loss: 6.6468 valuation loss: 6.8408\n",
      "step: 4800 training loss: 6.6418 valuation loss: 6.8303\n",
      "step: 4900 training loss: 6.5788 valuation loss: 6.8518\n",
      "step: 5000 training loss: 6.6204 valuation loss: 6.8405\n",
      "step: 5100 training loss: 6.5950 valuation loss: 6.8743\n",
      "step: 5200 training loss: 6.5689 valuation loss: 6.8495\n",
      "step: 5300 training loss: 6.5465 valuation loss: 6.8034\n",
      "step: 5400 training loss: 6.5385 valuation loss: 6.8478\n",
      "step: 5500 training loss: 6.5772 valuation loss: 6.7979\n",
      "step: 5600 training loss: 6.5276 valuation loss: 6.8383\n",
      "step: 5700 training loss: 6.5487 valuation loss: 6.7998\n",
      "step: 5800 training loss: 6.4757 valuation loss: 6.8649\n",
      "step: 5900 training loss: 6.4547 valuation loss: 6.7856\n",
      "step: 6000 training loss: 6.5036 valuation loss: 6.7813\n",
      "step: 6100 training loss: 6.4933 valuation loss: 6.8327\n",
      "step: 6200 training loss: 6.4267 valuation loss: 6.8266\n",
      "step: 6300 training loss: 6.3618 valuation loss: 6.7452\n",
      "step: 6400 training loss: 6.3913 valuation loss: 6.8007\n",
      "step: 6500 training loss: 6.4427 valuation loss: 6.7718\n",
      "step: 6600 training loss: 6.4000 valuation loss: 6.7738\n",
      "step: 6700 training loss: 6.4431 valuation loss: 6.7734\n",
      "step: 6800 training loss: 6.3557 valuation loss: 6.7647\n",
      "step: 6900 training loss: 6.3712 valuation loss: 6.7133\n",
      "step: 7000 training loss: 6.3253 valuation loss: 6.8071\n",
      "step: 7100 training loss: 6.4484 valuation loss: 6.8153\n",
      "step: 7200 training loss: 6.3707 valuation loss: 6.8139\n",
      "step: 7300 training loss: 6.3682 valuation loss: 6.7716\n",
      "step: 7400 training loss: 6.3353 valuation loss: 6.7723\n",
      "step: 7500 training loss: 6.3933 valuation loss: 6.7610\n",
      "step: 7600 training loss: 6.3092 valuation loss: 6.7894\n",
      "step: 7700 training loss: 6.3579 valuation loss: 6.8179\n",
      "step: 7800 training loss: 6.3142 valuation loss: 6.7547\n",
      "step: 7900 training loss: 6.3412 valuation loss: 6.7454\n",
      "step: 8000 training loss: 6.3668 valuation loss: 6.8076\n",
      "step: 8100 training loss: 6.2625 valuation loss: 6.7738\n",
      "step: 8200 training loss: 6.2693 valuation loss: 6.7280\n",
      "step: 8300 training loss: 6.3342 valuation loss: 6.7968\n",
      "step: 8400 training loss: 6.2420 valuation loss: 6.7921\n",
      "step: 8500 training loss: 6.3023 valuation loss: 6.8119\n",
      "step: 8600 training loss: 6.2879 valuation loss: 6.7389\n",
      "step: 8700 training loss: 6.2812 valuation loss: 6.7665\n",
      "step: 8800 training loss: 6.2856 valuation loss: 6.7801\n",
      "step: 8900 training loss: 6.3272 valuation loss: 6.8194\n",
      "step: 9000 training loss: 6.2457 valuation loss: 6.7962\n",
      "step: 9100 training loss: 6.2509 valuation loss: 6.7833\n",
      "step: 9200 training loss: 6.2489 valuation loss: 6.7064\n",
      "step: 9300 training loss: 6.2687 valuation loss: 6.7279\n",
      "step: 9400 training loss: 6.2434 valuation loss: 6.7887\n",
      "step: 9500 training loss: 6.2280 valuation loss: 6.7862\n",
      "step: 9600 training loss: 6.2058 valuation loss: 6.8115\n",
      "step: 9700 training loss: 6.2008 valuation loss: 6.7173\n",
      "step: 9800 training loss: 6.1896 valuation loss: 6.8324\n",
      "step: 9900 training loss: 6.1317 valuation loss: 6.7651\n",
      "step: 10000 training loss: 6.1834 valuation loss: 6.7048\n",
      "step: 10100 training loss: 6.2563 valuation loss: 6.7185\n",
      "step: 10200 training loss: 6.1708 valuation loss: 6.7062\n",
      "step: 10300 training loss: 6.1253 valuation loss: 6.7523\n",
      "step: 10400 training loss: 6.2022 valuation loss: 6.6892\n",
      "step: 10500 training loss: 6.1582 valuation loss: 6.7132\n",
      "step: 10600 training loss: 6.1330 valuation loss: 6.7875\n",
      "step: 10700 training loss: 6.1680 valuation loss: 6.7768\n",
      "step: 10800 training loss: 6.1598 valuation loss: 6.7196\n",
      "step: 10900 training loss: 6.1872 valuation loss: 6.7436\n",
      "step: 11000 training loss: 6.1653 valuation loss: 6.8330\n",
      "step: 11100 training loss: 6.1745 valuation loss: 6.7854\n",
      "step: 11200 training loss: 6.1380 valuation loss: 6.7884\n",
      "step: 11300 training loss: 6.1310 valuation loss: 6.7998\n",
      "step: 11400 training loss: 6.0947 valuation loss: 6.7673\n",
      "step: 11500 training loss: 6.1131 valuation loss: 6.7812\n",
      "step: 11600 training loss: 6.0844 valuation loss: 6.7730\n",
      "step: 11700 training loss: 6.1257 valuation loss: 6.7529\n",
      "step: 11800 training loss: 6.0664 valuation loss: 6.7406\n",
      "step: 11900 training loss: 6.1354 valuation loss: 6.8543\n",
      "step: 12000 training loss: 6.0793 valuation loss: 6.8075\n",
      "step: 12100 training loss: 6.0568 valuation loss: 6.7611\n",
      "step: 12200 training loss: 6.1359 valuation loss: 6.7265\n",
      "step: 12300 training loss: 6.0636 valuation loss: 6.7625\n",
      "step: 12400 training loss: 6.0855 valuation loss: 6.7760\n",
      "step: 12500 training loss: 6.1158 valuation loss: 6.7558\n",
      "step: 12600 training loss: 6.1001 valuation loss: 6.8187\n",
      "step: 12700 training loss: 6.0528 valuation loss: 6.7687\n",
      "step: 12800 training loss: 6.0811 valuation loss: 6.8171\n",
      "step: 12900 training loss: 5.9555 valuation loss: 6.8465\n",
      "step: 13000 training loss: 6.1267 valuation loss: 6.8367\n",
      "step: 13100 training loss: 6.0668 valuation loss: 6.8071\n",
      "step: 13200 training loss: 6.0401 valuation loss: 6.8538\n",
      "step: 13300 training loss: 6.0762 valuation loss: 6.8242\n",
      "step: 13400 training loss: 6.0120 valuation loss: 6.8226\n",
      "step: 13500 training loss: 6.0540 valuation loss: 6.8032\n",
      "step: 13600 training loss: 6.0264 valuation loss: 6.8031\n",
      "step: 13700 training loss: 6.0613 valuation loss: 6.8484\n",
      "step: 13800 training loss: 6.0277 valuation loss: 6.8139\n",
      "step: 13900 training loss: 6.0259 valuation loss: 6.8232\n",
      "step: 14000 training loss: 6.0238 valuation loss: 6.9068\n",
      "step: 14100 training loss: 5.9924 valuation loss: 6.8386\n",
      "step: 14200 training loss: 5.9977 valuation loss: 6.8233\n",
      "step: 14300 training loss: 6.0600 valuation loss: 6.8424\n",
      "step: 14400 training loss: 6.0305 valuation loss: 6.8454\n",
      "step: 14500 training loss: 6.0004 valuation loss: 6.8773\n",
      "step: 14600 training loss: 5.9958 valuation loss: 6.8112\n",
      "step: 14700 training loss: 6.0010 valuation loss: 6.8631\n",
      "step: 14800 training loss: 5.9636 valuation loss: 6.8317\n",
      "step: 14900 training loss: 5.9842 valuation loss: 6.9030\n",
      "step: 15000 training loss: 5.9567 valuation loss: 6.7877\n",
      "step: 15100 training loss: 5.9467 valuation loss: 6.8148\n",
      "step: 15200 training loss: 5.9691 valuation loss: 6.8807\n",
      "step: 15300 training loss: 5.9914 valuation loss: 6.8346\n",
      "step: 15400 training loss: 5.9994 valuation loss: 6.8216\n",
      "step: 15500 training loss: 6.0029 valuation loss: 6.8956\n",
      "step: 15600 training loss: 5.9657 valuation loss: 6.8614\n",
      "step: 15700 training loss: 5.9539 valuation loss: 6.8219\n",
      "step: 15800 training loss: 5.9198 valuation loss: 6.8740\n",
      "step: 15900 training loss: 5.9589 valuation loss: 6.8478\n",
      "step: 16000 training loss: 5.9157 valuation loss: 6.8385\n",
      "step: 16100 training loss: 5.9894 valuation loss: 6.9167\n",
      "step: 16200 training loss: 5.9747 valuation loss: 6.9105\n",
      "step: 16300 training loss: 5.8872 valuation loss: 6.8453\n",
      "step: 16400 training loss: 5.9435 valuation loss: 6.9568\n",
      "step: 16500 training loss: 5.9076 valuation loss: 6.8433\n",
      "step: 16600 training loss: 5.8808 valuation loss: 6.8975\n",
      "step: 16700 training loss: 5.9210 valuation loss: 6.9239\n",
      "step: 16800 training loss: 5.9480 valuation loss: 6.9416\n",
      "step: 16900 training loss: 5.8845 valuation loss: 6.9690\n",
      "step: 17000 training loss: 5.9181 valuation loss: 6.8625\n",
      "step: 17100 training loss: 5.8933 valuation loss: 6.8971\n",
      "step: 17200 training loss: 5.9097 valuation loss: 6.8725\n",
      "step: 17300 training loss: 5.8249 valuation loss: 6.9661\n",
      "step: 17400 training loss: 5.8735 valuation loss: 6.8805\n",
      "step: 17500 training loss: 5.8658 valuation loss: 6.8752\n",
      "step: 17600 training loss: 5.8911 valuation loss: 6.9410\n",
      "step: 17700 training loss: 5.8403 valuation loss: 6.9350\n",
      "step: 17800 training loss: 5.8878 valuation loss: 6.9672\n",
      "step: 17900 training loss: 5.8334 valuation loss: 6.9614\n",
      "step: 18000 training loss: 5.8535 valuation loss: 6.9468\n",
      "step: 18100 training loss: 5.8255 valuation loss: 6.9423\n",
      "step: 18200 training loss: 5.8098 valuation loss: 6.9293\n",
      "step: 18300 training loss: 5.9095 valuation loss: 6.8811\n",
      "step: 18400 training loss: 5.8153 valuation loss: 6.9312\n",
      "step: 18500 training loss: 5.8222 valuation loss: 6.9523\n",
      "step: 18600 training loss: 5.8372 valuation loss: 6.9061\n",
      "step: 18700 training loss: 5.8730 valuation loss: 6.9311\n",
      "step: 18800 training loss: 5.8230 valuation loss: 6.9453\n",
      "step: 18900 training loss: 5.8862 valuation loss: 6.9279\n",
      "step: 19000 training loss: 5.8203 valuation loss: 6.8669\n",
      "step: 19100 training loss: 5.7841 valuation loss: 6.9331\n",
      "step: 19200 training loss: 5.8495 valuation loss: 6.9498\n",
      "step: 19300 training loss: 5.7614 valuation loss: 6.9711\n",
      "step: 19400 training loss: 5.8398 valuation loss: 6.9712\n",
      "step: 19500 training loss: 5.7410 valuation loss: 7.0141\n",
      "step: 19600 training loss: 5.7675 valuation loss: 6.9635\n",
      "step: 19700 training loss: 5.7801 valuation loss: 6.9480\n",
      "step: 19800 training loss: 5.8337 valuation loss: 7.0100\n",
      "step: 19900 training loss: 5.8377 valuation loss: 7.0155\n",
      "step: 19999 training loss: 5.7538 valuation loss: 6.9386\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# creating vocab\n",
    "vocab_set = set()\n",
    "sentences = all_sentences\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        vocab_set.add(word)\n",
    "vocab = sorted(list(vocab_set))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "w2i = {w:i for i, w in enumerate(vocab)}\n",
    "side_spread = 2\n",
    "batch_size = 32\n",
    "n_embed = 10\n",
    "n_hidden = 10\n",
    "\n",
    "\n",
    "def get_context_n_target(words):\n",
    "    X, Y = [], []\n",
    "    if len(words) < side_spread + 1:\n",
    "        return X, Y\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        l = max(0, i - side_spread)\n",
    "        r = min(i + side_spread, len(words) - 1)\n",
    "        l = max(l, i - (r - i))\n",
    "        r = min(r, i + (i - l))\n",
    "        if i == r:\n",
    "            continue\n",
    "        x = list(map(w2i.get, [*words[l:i], *words[i+1:r+1]]))\n",
    "        X.append(x)\n",
    "\n",
    "        y = w2i[words[i]]\n",
    "        Y.append(y)\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def get_x_y(words):\n",
    "    window = 2*side_spread + 1\n",
    "    if len(words) < window:\n",
    "        return [], []\n",
    "    x, y = [], []\n",
    "    for i in range(len(words) - window + 1):\n",
    "        context_words = [*words[i:i+side_spread], *words[i+side_spread+1:i+2*side_spread+1]]\n",
    "        target_word = words[i+side_spread]\n",
    "        x.append([w2i[w] for w in context_words])\n",
    "        y.append(w2i[target_word])\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "\n",
    "\n",
    "def get_all_data(sentences: List[str]):\n",
    "    all_x, all_y = [], []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        x, y = get_x_y(words)\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "    return torch.tensor(all_x), torch.tensor(all_y)\n",
    "\n",
    "# splitting training and validation sets\n",
    "X, Y = get_all_data(all_sentences)\n",
    "# X, Y = get_context_n_target(words)\n",
    "n = int(0.9 * vocab_size)\n",
    "train_x, train_y = X[:n], Y[:n]\n",
    "val_x, val_y = X[n:], Y[n:]\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    x, y = (train_x, train_y) if 'train' == split else (val_x, val_y)\n",
    "    idx = torch.randint(len(x), (batch_size, ), generator=g)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target: torch.Tensor = None):\n",
    "        embeddings: torch.Tensor = self.embedding(x)\n",
    "        summed = embeddings.mean(1, keepdim=False) # (B, n_embed)\n",
    "        logits = self.proj(summed) # (B, vocab_size)\n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, eval_iters: int):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "epochs = 20000\n",
    "def training(eval_iters: int):\n",
    "    m = CBOW()\n",
    "    n_param = sum(p.numel() for p in m.parameters())\n",
    "    print(f'{n_param=}')\n",
    "    optim = torch.optim.SGD(m.parameters(), lr=1e-2)\n",
    "    for i in range(epochs):\n",
    "        # get batch\n",
    "        x, y = get_batch('train')\n",
    "\n",
    "        # forward\n",
    "        _, loss = m(x, y)\n",
    "        if i % eval_iters == 0 or i == epochs - 1:\n",
    "            losses = estimate_loss(m, eval_iters)\n",
    "            print(f\"step: {i} training loss: {losses['train']:.4f} valuation loss: {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "        # backward\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optim.step()\n",
    "    \n",
    "    return m\n",
    "\n",
    "m = training(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5176, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
