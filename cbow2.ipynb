{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "\n",
    "MIN_WORD_FREQUENCY = 50\n",
    "CBOW_N_WORDS = 2\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "BATCH_SIZE = 96\n",
    "DEVICE = torch.device('mps' if torch.has_mps else 'cpu')\n",
    "\n",
    "vocab_size = 4000\n",
    "n_embed = 300\n",
    "\n",
    "train_iter = to_map_style_dataset(WikiText2(split='train'))\n",
    "val_iter = to_map_style_dataset(WikiText2(split='valid'))\n",
    "\n",
    "def get_vocab(data_iter: Iterable, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, data_iter), MIN_WORD_FREQUENCY, ['<unk>'])\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab.to(DEVICE)\n",
    "\n",
    "\n",
    "# vocab(tokenizer('Hello.')) # vocab(list) -> list of indices\n",
    "\n",
    "\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    \"\"\"\n",
    "    Collate_fn for CBOW model to be used with Dataloader.\n",
    "    `batch` is expected to be list of text paragrahs.\n",
    "    \n",
    "    text_pipeline is a function that converts a list of words into indices\n",
    "\n",
    "    Context is represented as N=CBOW_N_WORDS past words \n",
    "    and N=CBOW_N_WORDS future words.\n",
    "    \n",
    "    Long paragraphs will be truncated to contain\n",
    "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
    "    \n",
    "    Each element in `batch_input` is N=CBOW_N_WORDS*2 context words.\n",
    "    Each element in `batch_output` is a middle word.\n",
    "    \"\"\"\n",
    "    xb, yb = [], []\n",
    "    for paragraph in batch:\n",
    "        text_indices = text_pipeline(paragraph)\n",
    "        window = CBOW_N_WORDS * 2 + 1\n",
    "        if len(text_indices) < window:\n",
    "            continue\n",
    "\n",
    "        text_indices = text_indices[:MAX_SEQUENCE_LENGTH] # why?\n",
    "\n",
    "        for i in range(len(text_indices) - CBOW_N_WORDS * 2):\n",
    "            x = [*text_indices[i:i+CBOW_N_WORDS], *text_indices[i+CBOW_N_WORDS+1:i + window]]\n",
    "            y = text_indices[i+CBOW_N_WORDS]\n",
    "            xb.append(x)\n",
    "            yb.append(y)\n",
    "    \n",
    "    return torch.tensor(xb, dtype=torch.long).to(DEVICE), torch.tensor(yb, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "\n",
    "# DataLoader splits data into batches of BATCH_SIZE paragraphs\n",
    "# high = 0\n",
    "# low = 1000\n",
    "# for x, y in dataloader:\n",
    "#     high = max(high, x.shape[0])\n",
    "#     low = min(low, x.shape[0])\n",
    "\n",
    "# low, high\n",
    "\n",
    "def get_dataloader_and_vocab(split: str, vocab=None):\n",
    "    data_iter = to_map_style_dataset(WikiText2(split=split))\n",
    "    tokenizer = get_tokenizer('basic_english', language='en')\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = get_vocab(data_iter, tokenizer)\n",
    "\n",
    "    text_pipeline = lambda paragraph: vocab(tokenizer(paragraph))\n",
    "    dataloader = DataLoader(data_iter, BATCH_SIZE, shuffle=True, collate_fn=partial(collate_cbow, text_pipeline=text_pipeline))\n",
    "    return dataloader, vocab\n",
    "\n",
    "\n",
    "train_dataloader, vocab = get_dataloader_and_vocab('train')\n",
    "# len(vocab), len(vocab.get_stoi())\n",
    "vocab['man']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab) -> None:\n",
    "        super().__init__()\n",
    "        vocab_size = len(vocab.get_stoi())\n",
    "        self.embeddings = nn.Embedding(vocab_size, n_embed, max_norm=1.0)\n",
    "        self.ln = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target: torch.Tensor = None):\n",
    "        # x = (B, 4)\n",
    "        x: torch.Tensor = self.embeddings(x) # (B, 4, 300)\n",
    "        x = x.mean(-2) # (B, 300)\n",
    "        logits = self.ln(x) # (B, vocab_size)\n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "def training(model: nn.Module, dataloader: DataLoader, optimizer, eval_iter: int = 100):\n",
    "    losses = []\n",
    "    for i, (x, target) in enumerate(dataloader, 1):\n",
    "        # forward\n",
    "        _, loss = model(x, target) # (B, vocab_size)\n",
    "        losses.append(loss.item())\n",
    "        if i % eval_iter == 0:\n",
    "            eval_loss = np.mean(losses)\n",
    "            print(f\"iteration: {i}, loss: {eval_loss:.4f}\")\n",
    "            losses = []\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "def validate(model: nn.Module, dataloader: DataLoader):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, target) in enumerate(dataloader):\n",
    "            _, loss = model(x, target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(losses)\n",
    "    print(f\"validation loss: {val_loss:.4f}\")\n",
    "    model.train()\n",
    "    return val_loss\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m         lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 23\u001b[0m model \u001b[39m=\u001b[39m train_and_eval(m, \u001b[39m5\u001b[39;49m, train_dataloader, val_dataloader)\n\u001b[1;32m     24\u001b[0m torch\u001b[39m.\u001b[39msave(model, \u001b[39m'\u001b[39m\u001b[39mparams/model.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[96], line 11\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, epochs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_eval\u001b[39m(model: nn\u001b[39m.\u001b[39mModule, epochs: \u001b[39mint\u001b[39m, train_dataloader, val_dataloader):\n\u001b[0;32m---> 11\u001b[0m     validate(model, val_dataloader)\n\u001b[1;32m     12\u001b[0m     optimzr \u001b[39m=\u001b[39m AdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.025\u001b[39m)\n\u001b[1;32m     13\u001b[0m     lr_scheduler \u001b[39m=\u001b[39m get_scheduler(optimzr, \u001b[39m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[95], line 43\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     41\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mfor\u001b[39;00m i, (x, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     44\u001b[0m         _, loss \u001b[39m=\u001b[39m model(x, target)\n\u001b[1;32m     45\u001b[0m         losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:629\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_name):\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[1;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader, vocab = get_dataloader_and_vocab('train')\n",
    "val_dataloader, _ = get_dataloader_and_vocab('valid', vocab=vocab)\n",
    "m = CBOW(vocab).to(DEVICE)\n",
    "\n",
    "def get_scheduler(optimizer, total_epochs: int):\n",
    "    l = lambda epoch: (total_epochs - epoch) / total_epochs\n",
    "    return LambdaLR(optimizer, lr_lambda=l, verbose=True)\n",
    "\n",
    "\n",
    "def train_and_eval(model: nn.Module, epochs: int, train_dataloader, val_dataloader):\n",
    "    validate(model, val_dataloader)\n",
    "    optimzr = AdamW(model.parameters(), lr=0.025)\n",
    "    lr_scheduler = get_scheduler(optimzr, 5)\n",
    "    for i in range(epochs):\n",
    "        print(f\"============= EPOCH{i+1} =============\")\n",
    "        model = training(model, train_dataloader, optimzr)\n",
    "        validate(model, val_dataloader)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_and_eval(m, 5, train_dataloader, val_dataloader)\n",
    "torch.save(model, 'params/model.pt')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('params/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
